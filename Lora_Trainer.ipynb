{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2ZBXMKS2-0u"
      },
      "source": [
        "# **Dataset Trainer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inc0BRV020xq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import toml\n",
        "import shutil\n",
        "import zipfile\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "TRAINER_COMMIT_HASH = \"89c30334016a27f1d890466151a61a46d8d1545e\"\n",
        "\n",
        "\n",
        "# These carry information from past executions\n",
        "if \"model_url\" in globals():\n",
        "  old_model_url = model_url\n",
        "else:\n",
        "  old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "  dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "  model_file = None\n",
        "\n",
        "\n",
        "#@markdown ## Setup\n",
        "dataset_name  = \"\" #@param {type:\"string\"}\n",
        "output_name = \"\" #@param {type:\"string\"}\n",
        "continue_from = \"\" #@param {type:\"string\"}\n",
        "model_url = \"https://civitai.com/api/download/models/127207?type=Model&format=SafeTensor&size=full&fp=fp32\" #@param {type:\"string\"}\n",
        "resolution = 768 #@param {type:\"slider\", min:512, max:1024, step:64}\n",
        "\n",
        "#@markdown ## Steps\n",
        "num_repeats = 5 #@param {type:\"number\"}\n",
        "num_epochs = 30 #@param {type:\"number\"}\n",
        "save_every_n_epochs = 2 #@param {type:\"number\"}\n",
        "keep_only_last_n_epochs = 20 #@param {type:\"number\"}\n",
        "train_batch_size = 3 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "\n",
        "#@markdown ## Learning\n",
        "unet_lr = 2e-4 #@param {type:\"number\"}\n",
        "text_encoder_lr = 1e-4 #@param {type:\"number\"}\n",
        "\n",
        "optimizer = \"AdamW8bit\" #@param [\"AdamW8bit\", \"DAdaptation\"]\n",
        "optimizer_args = None\n",
        "lr_scheduler = \"cosine_with_restarts\" #@param [\"constant\", \"cosine\", \"cosine_with_restarts\", \"constant_with_warmup\", \"linear\", \"polynomial\"]\n",
        "lr_scheduler_number = 3 #@param {type:\"number\"}\n",
        "lr_scheduler_num_cycles = lr_scheduler_number if lr_scheduler == \"cosine_with_restarts\" else 0\n",
        "lr_scheduler_power = lr_scheduler_number if lr_scheduler == \"polynomial\" else 0\n",
        "lr_warmup_ratio = 0.05 #@param {type:\"slider\", min:0.0, max:0.5, step:0.01}\n",
        "lr_warmup_steps = 0\n",
        "min_snr_gamma = True #@param {type:\"boolean\"}\n",
        "min_snr_gamma_value = 5.0 if min_snr_gamma else None\n",
        "\n",
        "#@markdown ## Network Structure\n",
        "lora_type = \"LoRA\" #@param [\"LoRA\", \"LoCon Lycoris\", \"LoHa Lycoris\"]\n",
        "network_dim = 128 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "network_alpha = 64 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "conv_dim = 8 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_alpha = 1 #@param {type:\"slider\", min:1, max:64, step:1}\n",
        "conv_compression = False #@param {type:\"boolean\"}\n",
        "\n",
        "if optimizer == \"DAdaptation\":\n",
        "  optimizer_args = [\"decouple=True\",\"weight_decay=0.2\"]\n",
        "  unet_lr = 0.5\n",
        "  text_encoder_lr = 0.5\n",
        "  lr_scheduler = \"constant_with_warmup\"\n",
        "\n",
        "network_module = \"lycoris.kohya\" if \"Lycoris\" in lora_type else \"networks.lora\"\n",
        "network_args = None if lora_type == \"LoRA\" else [\n",
        "  f\"conv_dim={conv_dim}\",\n",
        "  f\"conv_alpha={conv_alpha}\",\n",
        "]\n",
        "\n",
        "if \"Lycoris\" in lora_type:\n",
        "  network_args.append(f\"algo={'loha' if 'LoHa' in lora_type else 'lora'}\")\n",
        "  network_args.append(f\"disable_conv_cp={str(not conv_compression)}\")\n",
        "\n",
        "root_dir = \"/content\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\")\n",
        "images_folder = os.path.join(main_dir, \"datasets\", dataset_name)\n",
        "output_folder = os.path.join(main_dir, \"output\", output_name)\n",
        "config_folder = os.path.join(main_dir, \"config\", output_name)\n",
        "log_folder    = os.path.join(main_dir, \"log\")\n",
        "metadata_file = os.path.join(images_folder, \"metadata.json\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "\n",
        "def setup_trainer():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone https://github.com/kohya-ss/sd-scripts {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  !git reset --hard {TRAINER_COMMIT_HASH}\n",
        "\n",
        "  # apply custom patches\n",
        "  !wget -O - https://raw.githubusercontent.com/recris/kohya-colab/main/masked_loss_blur.patch | git apply -v\n",
        "  !wget https://raw.githubusercontent.com/recris/kohya-colab/main/requirements.txt -q -O requirements.txt\n",
        "  !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
        "  !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 unzip\n",
        "  !aria2c https://huggingface.co/munsy0227/kohya-colab-file/resolve/main/xformers-0.0.21.dev564-cp310-cp310-manylinux2014_x86_64.whl\n",
        "  !mv bd656faa4ec396d1bb365f7b58d2831eaac129ac82076d20084f74dcd0399b08 xformers-0.0.21.dev564-cp310-cp310-manylinux2014_x86_64.whl\n",
        "  !pip -q install --upgrade -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "  !pip -q install --upgrade xformers-0.0.21.dev564-cp310-cp310-manylinux2014_x86_64.whl\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "\n",
        "def validate_dataset():\n",
        "  global lr_warmup_steps, lr_warmup_ratio, caption_extension\n",
        "  supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\")\n",
        "\n",
        "  print(\"\\n💿 Checking dataset...\")\n",
        "  if not dataset_name.strip() or any(c in dataset_name for c in \" .()\\\"'\\\\/\"):\n",
        "    print(\"💥 Error: Please choose a valid project name.\")\n",
        "    return\n",
        "\n",
        "  folders = [images_folder]\n",
        "  files = os.listdir(images_folder)\n",
        "  images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "  for folder in folders:\n",
        "    if not os.path.exists(folder):\n",
        "      print(f\"💥 Error: The folder {folder.replace('/content/drive/', '')} doesn't exist.\")\n",
        "      return\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    if not img:\n",
        "      print(f\"💥 Error: Your {folder.replace('/content/drive/', '')} folder is empty.\")\n",
        "      return\n",
        "\n",
        "  if not os.path.exists(metadata_file):\n",
        "    print(f\"💥 Error: Images metadata file does not exist.\")\n",
        "    return\n",
        "\n",
        "  if continue_from and not (continue_from.endswith(\".safetensors\") and os.path.exists(continue_from)):\n",
        "    print(f\"💥 Error: Invalid path to existing Lora. Example: /content/drive/MyDrive/lora_training/checkpoints/example.safetensors\")\n",
        "    return\n",
        "\n",
        "  pre_steps_per_epoch = sum(img*rep for (img, rep) in images_repeats.values())\n",
        "  steps_per_epoch = pre_steps_per_epoch/train_batch_size\n",
        "  total_steps = int(num_epochs*steps_per_epoch)\n",
        "  estimated_epochs = int(total_steps/steps_per_epoch)\n",
        "  lr_warmup_steps = int(total_steps*lr_warmup_ratio)\n",
        "\n",
        "  for folder, (img, rep) in images_repeats.items():\n",
        "    print(\"📁\"+folder.replace(\"/content/drive/\", \"\"))\n",
        "    print(f\"📈 Found {img} images with {rep} repeats, equaling {img*rep} steps.\")\n",
        "\n",
        "  print(f\"📉 Divide {pre_steps_per_epoch} steps by {train_batch_size} batch size to get {steps_per_epoch} steps per epoch.\")\n",
        "  print(f\"🔮 There will be {num_epochs} epochs, for around {total_steps} total training steps.\")\n",
        "\n",
        "  if total_steps > 20000:\n",
        "    print(\"💥 Error: Your total steps are too high. You probably made a mistake. Aborting...\")\n",
        "    return\n",
        "\n",
        "  return True\n",
        "\n",
        "\n",
        "def create_config():\n",
        "  global dataset_config_file, config_file, model_file\n",
        "\n",
        "  config_dict = {\n",
        "    \"additional_network_arguments\": {\n",
        "      \"unet_lr\": unet_lr,\n",
        "      \"text_encoder_lr\": text_encoder_lr,\n",
        "      \"network_dim\": network_dim,\n",
        "      \"network_alpha\": network_alpha,\n",
        "      \"network_module\": network_module,\n",
        "      \"network_args\": network_args,\n",
        "      \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "      \"network_weights\": continue_from if continue_from else None,\n",
        "      \"network_dropout\": 0.15,\n",
        "      \"scale_weight_norms\": 2.0\n",
        "    },\n",
        "    \"optimizer_arguments\": {\n",
        "      \"learning_rate\": unet_lr,\n",
        "      \"lr_scheduler\": lr_scheduler,\n",
        "      \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "      \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "      \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "      \"optimizer_type\": optimizer,\n",
        "      \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "    },\n",
        "    \"training_arguments\": {\n",
        "      \"max_train_steps\": None,\n",
        "      \"max_train_epochs\": num_epochs,\n",
        "      \"save_every_n_epochs\": save_every_n_epochs,\n",
        "      \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "      \"train_batch_size\": train_batch_size,\n",
        "      \"noise_offset\": None,\n",
        "      \"clip_skip\": 1,\n",
        "      \"min_snr_gamma\": min_snr_gamma_value,\n",
        "      \"weighted_captions\": False,\n",
        "      \"seed\": 42,\n",
        "      \"max_token_length\": 225,\n",
        "      \"xformers\": True,\n",
        "      \"lowram\": True,\n",
        "      \"max_data_loader_n_workers\": 8,\n",
        "      \"persistent_data_loader_workers\": True,\n",
        "      \"save_precision\": \"fp16\",\n",
        "      \"mixed_precision\": \"fp16\",\n",
        "      \"output_dir\": output_folder,\n",
        "      \"logging_dir\": log_folder,\n",
        "      \"output_name\": output_name,\n",
        "      \"log_prefix\": output_name,\n",
        "      \"save_state\": False,\n",
        "      \"save_last_n_epochs_state\": None,\n",
        "      \"resume\": None,\n",
        "      \"masked_loss\": True\n",
        "    },\n",
        "    \"model_arguments\": {\n",
        "      \"pretrained_model_name_or_path\": model_file,\n",
        "      \"v2\": False,\n",
        "      \"v_parameterization\": None,\n",
        "    },\n",
        "    \"saving_arguments\": {\n",
        "      \"save_model_as\": \"safetensors\",\n",
        "    },\n",
        "    \"dreambooth_arguments\": {\n",
        "      \"prior_loss_weight\": 1.0,\n",
        "    },\n",
        "    \"dataset_arguments\": {\n",
        "      \"cache_latents\": True,\n",
        "    },\n",
        "  }\n",
        "\n",
        "  for key in config_dict:\n",
        "    if isinstance(config_dict[key], dict):\n",
        "      config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "  with open(config_file, \"w\") as f:\n",
        "    f.write(toml.dumps(config_dict))\n",
        "\n",
        "  print(f\"\\n📄 Config saved to {config_file}\")\n",
        "\n",
        "  dataset_config_dict = {\n",
        "    \"general\": {\n",
        "      \"resolution\": resolution,\n",
        "      \"shuffle_caption\": False,\n",
        "      \"keep_tokens\": 1,\n",
        "      \"flip_aug\": False,\n",
        "      \"caption_extension\": \".txt\",\n",
        "      \"enable_bucket\": True,\n",
        "      \"bucket_reso_steps\": 64,\n",
        "      \"bucket_no_upscale\": False,\n",
        "      \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "      \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "    },\n",
        "    \"datasets\": [\n",
        "      {\n",
        "        \"subsets\": [\n",
        "          {\n",
        "            \"num_repeats\": num_repeats,\n",
        "            \"image_dir\": images_folder,\n",
        "            \"metadata_file\": metadata_file,\n",
        "            \"class_tokens\": None\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "\n",
        "  for key in dataset_config_dict:\n",
        "    if isinstance(dataset_config_dict[key], dict):\n",
        "      dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "  with open(dataset_config_file, \"w\") as f:\n",
        "    f.write(toml.dumps(dataset_config_dict))\n",
        "\n",
        "  print(f\"📄 Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "\n",
        "def download_model():\n",
        "  global old_model_url, model_url, model_file\n",
        "  real_model_url = model_url.strip()\n",
        "\n",
        "  if real_model_url.lower().endswith((\".ckpt\", \".safetensors\")):\n",
        "    model_file = f\"/content{real_model_url[real_model_url.rfind('/'):]}\"\n",
        "  else:\n",
        "    model_file = \"/content/downloaded_model.safetensors\"\n",
        "    if os.path.exists(model_file):\n",
        "      !rm \"{model_file}\"\n",
        "\n",
        "  if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "    real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "  elif m := re.search(r\"(?:https?://)?(?:www\\.)?civitai\\.com/models/([0-9]+)\", model_url):\n",
        "    real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "\n",
        "  !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "  if model_file.lower().endswith(\".safetensors\"):\n",
        "    from safetensors.torch import load_file as load_safetensors\n",
        "    try:\n",
        "      test = load_safetensors(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      #if \"HeaderTooLarge\" in str(e):\n",
        "      new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "      !mv \"{model_file}\" \"{new_model_file}\"\n",
        "      model_file = new_model_file\n",
        "      print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "  if model_file.lower().endswith(\".ckpt\"):\n",
        "    from torch import load as load_ckpt\n",
        "    try:\n",
        "      test = load_ckpt(model_file)\n",
        "      del test\n",
        "    except Exception as e:\n",
        "      return False\n",
        "\n",
        "  return True\n",
        "\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"📂 Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\n🏭 Installing dependencies...\\n\")\n",
        "    t0 = time()\n",
        "    setup_trainer()\n",
        "    t1 = time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\n✅ Installation finished in {int(t1-t0)} seconds.\")\n",
        "  else:\n",
        "    print(\"\\n✅ Dependencies already installed.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\n🔄 Downloading model...\")\n",
        "    if not download_model():\n",
        "      print(\"\\n💥 Error: The model you selected is invalid or corrupted, or couldn't be downloaded. You can use a civitai or huggingface link, or any direct download link.\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\n🔄 Model already downloaded.\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\n⭐ Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    display(Markdown(\"### ✅ Done!\"))\n",
        "\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}